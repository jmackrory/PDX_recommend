{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Recommendation Engines\n",
    "\n",
    "This carries on the work from \"recommend.ipynb\", which did some exploratory analysis, and tried to develop some computation methods based\n",
    "on using dense matrices built via the pivot table function.  This work will try to work with sparse matrices, and write the functions that are required.  In particular, I intend to attempt both user-user and item-item collaborative filtering.\n",
    "\n",
    "I would also like to attempt latent factor analysis.  The existing routines for matrix factorization reconstruct an entire matrix, rather than just a sub-portion.  I might try using stochastic gradient descent to carry out this factorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Libraries\n",
    "#standard library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#save graphics as pdf too (for less revolting exported plots)\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png', 'pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 600 seconds\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(600000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%autosave 600 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#read in the data.  (13MB or so)\n",
    "#(N.B. I put Matt's header on it's own line, which is skipped, and added the UserID)\n",
    "#initial playing data\n",
    "#frequent users\n",
    "#df=pd.read_csv('data/boardgame-frequent-users.csv',skiprows=1)\n",
    "\n",
    "#full matrix (2E5 users, 400 games)\n",
    "df=pd.read_csv('data/boardgame-users.csv',skiprows=1)\n",
    "df.columns=('userID','gameID','rating')\n",
    "\n",
    "detail_df=pd.read_csv('data/boardgame-details.csv',index_col=0)\n",
    "name_dict=detail_df['title'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nusers:193504, Ngames:402\n"
     ]
    }
   ],
   "source": [
    "#find unique entries\n",
    "users=df['userID'].unique()\n",
    "games=df['gameID'].unique()\n",
    "\n",
    "Nusers = len(users)\n",
    "Ngames = len(games)\n",
    "print('Nusers:{}, Ngames:{}'.format(Nusers,Ngames))\n",
    "\n",
    "#sort the list.\n",
    "users.sort()\n",
    "games.sort()\n",
    "\n",
    "#make a dict of user/games for correspondence between IDs and row/columns.\n",
    "user_dict=dict(zip(users,np.arange(Nusers)))\n",
    "game_dict=dict(zip(games,np.arange(Ngames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#find changes in user IDs by looking at first difference.  (use fact that users are sorted together)\n",
    "user_diff = np.diff(df['userID'])\n",
    "diff_msk= (user_diff!=0)\n",
    "#find entries where userID changes.  indices run from 0:len(df)-1 due to difference.\n",
    "#Must add 1 to get correct index\n",
    "diff_ind=np.arange(len(df)-1)[diff_msk]+1\n",
    "#put in first index at zero, and last index at df\n",
    "user_ind=np.insert(diff_ind,0,0,axis=0)\n",
    "user_ind=np.append(user_ind,[len(df)],axis=0)\n",
    "user_change=df['userID'][user_ind]\n",
    "#now find single indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#extract indices\n",
    "score_row = np.array([user_dict[v] for v in df['userID'].values])\n",
    "score_col = np.array([game_dict[v] for v in df['gameID'].values])\n",
    "score_val = df['rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On user 190000 of 193504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On user 170000 of 193504\n",
      "On user 180000 of 193504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On user 160000 of 193504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On user 150000 of 193504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On user 130000 of 193504\n",
      "On user 140000 of 193504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On user 120000 of 193504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On user 100000 of 193504\n",
      "On user 110000 of 193504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On user 90000 of 193504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "On user 80000 of 193504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On user 60000 of 193504\n",
      "On user 70000 of 193504"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On user 50000 of 193504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On user 30000 of 193504\n",
      "On user 40000 of 193504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On user 20000 of 193504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On user 10000 of 193504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On user 0 of 193504\n"
     ]
    }
   ],
   "source": [
    "#subtract off mean user score from user score.\n",
    "\n",
    "user_mean=np.zeros(Nusers)\n",
    "score_val_u=np.zeros(score_val.shape)\n",
    "single_users=[]\n",
    "low_users=[]\n",
    "\n",
    "#with the user indices, now takes linear time.  super fast.\n",
    "for i in range(Nusers):\n",
    "    sl = slice(user_ind[i],user_ind[i+1])\n",
    "    mu=np.mean(score_val[sl])\n",
    "    user_mean[i]=mu\n",
    "    score_val_u[sl] = score_val[sl]-mu\n",
    "    #grab single users.\n",
    "    nreviews=user_ind[i+1]-user_ind[i]\n",
    "    if (nreviews==1):\n",
    "        single_users.append(i)\n",
    "    if (nreviews<=5):\n",
    "        low_users.append(i)\n",
    "    if (i%10000==0):\n",
    "        print('On user {} of {}'.format(i,Nusers))\n",
    "\n",
    "#subtract off mean game score from user score.\n",
    "game_mean=np.zeros(Ngames)\n",
    "score_val_g=np.zeros(score_val.shape)\n",
    "#do the simple way, since relatively only 400 games\n",
    "for i in range(Ngames):\n",
    "    msk= score_col==i\n",
    "    mu=np.mean(score_val[msk])\n",
    "    game_mean[i]=mu\n",
    "    score_val_g[msk] = score_val[msk]-mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of single users:25191 Number of users with fewer than 5 reviews:61886\n"
     ]
    }
   ],
   "source": [
    "#Find number of \"low users, and single review users\"\n",
    "print('Number of single users:{} Number of users with fewer than 5 reviews:{}'.format(len(single_users),len(low_users)))\n",
    "#So around a quarter of the dataset has 5 or fewer reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now we can randomly split the dataset into testing/validation sets.  \n",
    "Matt suggested taking care to ensure no reviewers were with left with no reviews.  \n",
    "What if we ignore that? For the KNN approach, I was going to weight together the recommendations with the mean, so very little effect.  For the ALS-matrix factorization approach, these would have to get dropped anyway (as they are all None/NAN).  So yeah, it makes sense to filter those out.   This does mean that you can test your methods for new users, then again the recommendations were probably just median recommendations anyway.  For the moment, let's ignore that.  (And ultimately, I don't care about people with only a single review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4118900,) (1029724,)\n",
      "[      3       4      19 ..., 5148615 5148619 5148620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4118899,) (1029725,)\n",
      "[     10      13      14 ..., 5148609 5148611 5148621]\n",
      "(4118899,) (1029725,)\n",
      "[      0      11      15 ..., 5148614 5148616 5148622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4118899,) (1029725,)\n",
      "[      2       7       8 ..., 5148608 5148617 5148623]\n",
      "(4118899,) (1029725,)\n",
      "[      1       5       6 ..., 5148599 5148604 5148618]\n"
     ]
    }
   ],
   "source": [
    "kf=KFold(n_splits=5,random_state=3413, shuffle=True)\n",
    "split_ind=kf.split(np.arange(len(df)))\n",
    "train_list=[]\n",
    "test_list=[]\n",
    "for train_ind,test_ind in split_ind:\n",
    "    train_list.append(train_ind)\n",
    "    test_list.append(test_ind)\n",
    "    print(train_ind.shape,test_ind.shape)\n",
    "    print(test_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0,       1,       3, ..., 5148620, 5148621, 5148622])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193504, 402)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193503, 402)\n"
     ]
    }
   ],
   "source": [
    "#make a coordinate sparse matrix, then convert it to compressed row form.\n",
    "def get_sparse_matrix(ind):\n",
    "    score_coo = sparse.coo_matrix((score_val_g[ind],(score_row[ind],score_col[ind])))\n",
    "    #convert to either compressed row or column matrix. (csr, csc)\n",
    "    #use R fast row slicing, and fast arithmetic\n",
    "    print(score_coo.shape)\n",
    "    score_R = score_coo.tocsr()\n",
    "    return score_R\n",
    "score_train=get_sparse_matrix(train_list[0])\n",
    "score_test = get_sparse_matrix(test_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# PCA\n",
    "\n",
    "Let's try doing a PCA for dimensionality reduction.  Also do some clustering, and maybe even some filtering.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-ea5a39208c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# %debug --breakpoint /home/jonathan/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/truncated_svd.py:187\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mscore_pca\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_R\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jonathan/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    172\u001b[0m             U, Sigma, VT = randomized_svd(X, self.n_components,\n\u001b[1;32m    173\u001b[0m                                           \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                                           random_state=random_state)\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown algorithm %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jonathan/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_svd\u001b[0;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     Q = randomized_range_finder(M, n_random, n_iter,\n\u001b[0;32m--> 326\u001b[0;31m                                 power_iteration_normalizer, random_state)\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;31m# project M to the (k + p) dimensional space using the basis vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jonathan/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_range_finder\u001b[0;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LU'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'QR'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'economic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jonathan/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \"\"\"\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"toarray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jonathan/anaconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jonathan/anaconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_multivector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m# csr_matvecs or csc_matvecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sparsetools\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_matvecs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb OFF\n",
    "svd=TruncatedSVD(n_components=20,random_state=439,n_iter=1000)\n",
    "# %debug --breakpoint /home/jonathan/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/truncated_svd.py:187\n",
    "\n",
    "score_pca=svd.fit_transform(score_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "comp=svd.components_\n",
    "exp_var=svd.explained_variance_\n",
    "#reconstruct the matrix by adding up components again.\n",
    "score_recon=np.dot(score_pca,comp)\n",
    "score_full=score_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/pdf": "JVBERi0xLjQKJazcIKu6CjEgMCBvYmoKPDwgL1BhZ2VzIDIgMCBSIC9UeXBlIC9DYXRhbG9nID4+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\nNTczOQolJUVPRgo=\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6acd835550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(exp_var,'-x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.592313877564\n",
      "[[ 0.38650579  0.32182079  0.13009365  0.1543331  -0.10450587]\n",
      " [ 0.40596635  0.00233083 -0.20857872 -1.367775   -0.15081522]\n",
      " [-0.25777031 -0.19457546 -0.01022145 -0.60538809 -0.53606796]\n",
      " [-0.0104767   0.57551235 -0.04993931  0.08202462  0.03743651]\n",
      " [ 0.58054752  0.36785332  0.24405721  1.08269932 -0.16995015]]\n"
     ]
    }
   ],
   "source": [
    "diff=score_recon-score_full\n",
    "cost=np.sum(np.abs(diff)/(Nusers*Ngames))\n",
    "print(cost)\n",
    "print(diff[0:5,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Plot the components against one another.\n",
    "def plot_comp(score_pca):\n",
    "    nc=score_pca.shape[1]\n",
    "    print(nc)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(nc):\n",
    "        for j in range(i,nc):\n",
    "            plt.subplot(nc,nc,j+i*nc+1)\n",
    "            #plt.scatter(score_pca[:,i],score_pca[:,j],marker='x')\n",
    "\n",
    "    plt.show()            \n",
    "    return\n",
    "#will kill your computer \n",
    "#plot_comp(score_pca)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Evidently, there is not much to differentiate users based solely on their scores.  (Again, other people at the workshop commented on exactly this - they suggested there was more variation for a PCA based on games.)  Let us try the same PCA on the transposed matrix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Once again, this seems to have just found a big blob, which suggests that clustering is not meaningful.  (These are for the normalized scores where the average score for each game has been subtracted, before carrying out the PCA).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Item-Item filtering (via KNN)\n",
    "\n",
    "A simple recommendation algorithm could be designed by using the reduced dimensions to carry out k-nearest neighbours.  It would be slow for large matrices.  Let's try this now for the reduced data set after the SVD. \n",
    "\n",
    "So I was initially writing my own code for this, but it seems that scikit-learn already has a nearest neighbour regressor (and nearest radius regressor) via sklearn.neighbours.RadiusNeighboursRegressor, and sklearn.neighbours.NearestNeighboursRegressor.  One concern is since I am using the scores (and want to predict the scores, do I need to feed it training data? Yes, I just gave it the scores to reproduce.  No problem.\n",
    "\n",
    "Note that this is currently based on finding the nearest vectors via\n",
    "$\\text{min}_j|\\vec{v}_i-\\vec{v}_j|^2$, where we replace $\\vec{v}$ with its k-nearest neighbors:\n",
    "\\begin{align}\n",
    "    \\bar{v}_i &= \\sum_{i=1}^k c_{ij}v_j,\\\\\n",
    "    \\text{where } c_{ij} &= \\frac{\\vec{v}_i\\cdot \\vec{v}_j}{|\\vec{v}|^2}\n",
    "\\end{equation}\n",
    "Note that this unfortunately would underweight vectors which have the same scores as $v_i$, but have more information.\n",
    "If I have $v = (1,nan,nan,-1,nan)$, that is closer to a vector like $v_{bad}=(-1,nan,nan,0,nan)$, than it is to a vector $v_{good}=(1,1,-2,-1,2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tmin=score_train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x402 sparse matrix of type '<class 'numpy.float64'>'\n\twith 24 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-5948107e10d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mKnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;31m#k_ind=get_k_indices(score_train[0:10000], Knn,Nbatch=100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0mr_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rad_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-205-5948107e10d2>\u001b[0m in \u001b[0;36mget_rad_indices\u001b[0;34m(score_mat, score_norm_inv, rad, ind)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mnvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m#the following sorts the array based on the values in the first row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0midx_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdist2\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mrad\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0midx_vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "Knn = 100\n",
    "#So this currently gets all of the nearest neighbors.  That's daft.\n",
    "#Make function to just get one at a time. \n",
    "# def get_knn_indices(score_mat,Knn,Nbatch=100):\n",
    "#     \"\"\"get_knn_indices\n",
    "#     Extracts nearest k indices nearest vector for each row vector.\n",
    "#     Inputs\n",
    "#        score_mat: sparse matrix, where each row is a score vector. \n",
    "#                   (Best if CSR format)\n",
    "#        Knn: number of nearest neighbours to keep\n",
    "#     \"\"\"\n",
    "#     Nrows=score_mat.shape[0]\n",
    "#     nvec = np.arange(Nrows).astype(int)\n",
    "#     idx_array = np.zeros([Nrows,Knn])\n",
    "#     #make an array of indices of nearest neighbours.\n",
    "#     score_T = score_mat.transpose()\n",
    "\n",
    "#     dnorm2=score_mat.multiply(score_mat).sum(axis=1)\n",
    "#     for i in range(Nrows):\n",
    "#         #calculate |v_i - v_j|^2 for all vectors.   \n",
    "#         if (i%Nbatch==0):\n",
    "#             #print('Iter {} of {}'.format(i,Nrows))\n",
    "#             #compute batch of dot-products \n",
    "#             dot_batch = score_mat.dot(score_T[:,i:i+Nbatch]).tocsc()\n",
    "#         dist2=dnorm2[i] - 2*dot_batch[:,i%Nbatch] + dnorm2\n",
    "#         #ignore the values for any identical vectors \n",
    "#         dist2[abs(dist2)<=1E-12]=dist2.max()\n",
    "#         dmat=np.array([dist2.T,nvec])\n",
    "#         #the following sorts the array based on the values in the first row\n",
    "#         idx = np.argsort(dmat[0])\n",
    "#         #grab first Knn entries since sorted in ascending order, and want nearest.\n",
    "#         idx_array[i,:]=idx[0,:Knn]\n",
    "#     return idx_array\n",
    "\n",
    "def get_cosine_distances(score_mat,ind,score_norm_inv):\n",
    "    \"\"\"calculate distances between vectors.\n",
    "    Inputs:\n",
    "    score_mat - CSR matrix of row vectors\n",
    "    ind - specific index for vector v_i\n",
    "    score_norm_inv - inverse euclidean norms for each vector.\n",
    "    Return:\n",
    "    dot_prods - vector of cosines between v_i, and all other vectors\n",
    "    \"\"\"\n",
    "    #make an array of indices of nearest neighbours.\n",
    "    score_vec = score_mat[ind].transpose()*score_norm_inv[ind]\n",
    "    #note that this can be a bit inefficient (may be better to do as batch process with a number of columns.\n",
    "    dot_prods=score_mat.dot(score_vec)\n",
    "    dot_prods = np.multiply(dot_prods,score_norm_inv)\n",
    "    return dot_prods\n",
    "\n",
    "# def calc_norms(score_mat):\n",
    "#     \"\"\"calc_norms\n",
    "#     Compute euclidean norm for each row in CSR matrix\n",
    "#     \"\"\"\n",
    "#     dnorm=score_mat.multiply(score_mat).sum(axis=1)\n",
    "#     dnorm=np.sqrt(dnorm)\n",
    "#     return dnorm\n",
    "\n",
    "def calc_inverse_norm(score_mat):\n",
    "    \"\"\"calc_inverse_norm\n",
    "    Compute inverse of euclidean norm of reach row. i.e. (1/|R_i|) where\n",
    "    R_i is the i_th row in the matrix.\n",
    "    Treat 0 length vectors as having a small length (1E-10).\n",
    "    \"\"\"\n",
    "    dnorm=score_mat.multiply(score_mat).sum(axis=1)\n",
    "    inorm=1.0/np.maximum(np.sqrt(dnorm),1E-10)\n",
    "    return inorm\n",
    "\n",
    "def get_knn_indices(score_mat,Knn,score_norm_inv,ind):\n",
    "    \"\"\"get_knn_indices\n",
    "    Extracts nearest k indices nearest vector for a particular row vector.\n",
    "    Inputs\n",
    "       score_mat: sparse matrix, where each row is a score vector. \n",
    "                  (Best if CSR format)\n",
    "       Knn: number of nearest neighbours to keep\n",
    "       inorm: list of inverse euclidean lengths of each vector. (1/|v|)\n",
    "       ind: index of vector to find\n",
    "    \"\"\"\n",
    "\n",
    "    dist2=get_cosine_distances(score_mat,ind,score_norm_inv)\n",
    "    #the following sorts the array based on the values in the first row\n",
    "    #then grab first Knn entries since sorted in ascending order, and want nearest.\n",
    "    Nrows=score_mat.shape[0]\n",
    "    nvec = np.arange(Nrows).astype(int)\n",
    "    dmat=np.array([dist2.T,nvec])\n",
    "    idx = np.argsort(dmat[0])\n",
    "    idx_vec=idx[0,:Knn]\n",
    "\n",
    "    return idx_vec\n",
    "\n",
    "\n",
    "def get_rad_indices(score_mat,score_norm_inv,rad,ind):\n",
    "    \"\"\"get_knn_index\n",
    "    Extracts nearest k indices nearest vector for a particular row vector.\n",
    "    Inputs:\n",
    "       score_mat: sparse matrix, where each row is a score vector. \n",
    "                  (Best if CSR format)\n",
    "       rad: radius of ball to keep\n",
    "       norm_inv: list of euclidean lengths of each vector.\n",
    "       ind: index of vector to find\n",
    "    \"\"\"\n",
    "    dist2=get_cosine_distances(score_mat,ind,score_norm_inv)    \n",
    "    nvec = np.arange(score_mat.shape[0]).astype(int)    \n",
    "    #the following sorts the array based on the values in the first row\n",
    "    idx_vec = nvec[dist2>rad*rad]\n",
    "    return idx_vec\n",
    "\n",
    "#this is super slow.  (a full KNN search takes a long time basically.  Not needed - just do in time/as required?)\n",
    "%pdb off\n",
    "Knn = 100\n",
    "#k_ind=get_k_indices(score_train[0:10000], Knn,Nbatch=100)\n",
    "r_ind = get_rad_indices(score_train,inorm,rad=0.5,ind=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, ..., False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array((dvec>0.5)).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dnorm=calc_norms(score_train)\n",
    "inorm=calc_inverse_norm(score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 5.96740119]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inorm[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dvec=get_cosine_distances(score_train,0,inorm)\n",
    "np.sum(dvec>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#compute average vector up vectors.\n",
    "def kvec(score_mat,idx_array,ind):\n",
    "    \"\"\"kvec \n",
    "    Given the nearest indices, actually go and calculate the \n",
    "    averaged knn vector.  Note this would be dense, so only do this on demand. \n",
    "    \"\"\"\n",
    "    Ncol=score_mat.shape[1]\n",
    "    #vector we want to reconstruct via nearby vectors.\n",
    "    ai = score_mat[ind]\n",
    "    kvec=np.zeros((1,Ncol))\n",
    "    for idx in idx_array[ind]:\n",
    "        vj = score_mat[idx]\n",
    "        v_norm=np.dot(vj,vj.T)\n",
    "        kvec += np.dot(ai,vj.T)/v_norm*(vj)\n",
    "    return kvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   1.59723861e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,  -1.37953885e-03,   0.00000000e+00,   0.00000000e+00,\n         -6.97038128e-01,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   2.49184799e-01,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,  -6.82043752e-02,\n          0.00000000e+00,   0.00000000e+00,   3.49024392e-01,   0.00000000e+00,\n          0.00000000e+00,  -7.62263377e-01,   0.00000000e+00,   0.00000000e+00,\n          2.04205928e+00,   0.00000000e+00,   0.00000000e+00,   8.38611642e-01,\n          0.00000000e+00,   2.86335005e-02,   0.00000000e+00,   1.20597684e-01,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   1.61010230e-01,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          6.19786154e+01,  -4.49778276e-01,   0.00000000e+00,   0.00000000e+00,\n          2.44680912e-01,   4.00046525e-01,   0.00000000e+00,   4.67297844e-01,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   3.10299309e-01,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   9.91464357e-02,   0.00000000e+00,\n          0.00000000e+00,  -9.59533112e-01,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   3.83235247e+00,\n          0.00000000e+00,  -4.95560765e-01,   0.00000000e+00,   6.45864540e-01,\n          1.24797649e-01,   0.00000000e+00,   1.24729566e-01,   4.97999092e-03,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          2.22505612e-01,   1.46826106e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,  -3.50736975e-01,   0.00000000e+00,\n          1.50213948e+00,   0.00000000e+00,  -4.91572138e-02,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,  -5.16883538e-01,   4.67164254e-01,  -3.76845535e-01,\n          0.00000000e+00,   5.20056312e-01,   2.49850647e-01,   0.00000000e+00,\n          0.00000000e+00,  -1.47016822e-01,  -3.22930479e-01,   0.00000000e+00,\n          0.00000000e+00,   4.03506057e-01,   0.00000000e+00,   1.49831031e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          1.25108108e-01,   0.00000000e+00,   2.34437933e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   1.22925953e+00,\n          4.59581426e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   4.75137194e-01,   0.00000000e+00,   0.00000000e+00,\n          5.55884332e-01,   0.00000000e+00,   0.00000000e+00,   3.39067138e-01,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   2.26682295e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   6.84739702e-01,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   6.70514325e-01,\n          0.00000000e+00,   0.00000000e+00,  -2.67361658e-01,   0.00000000e+00,\n          0.00000000e+00,  -1.58250070e-01,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   9.44350723e-01,   5.37416834e-02,\n          1.81947961e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   5.19239733e+00,\n          0.00000000e+00,   0.00000000e+00,   7.60447524e-01,   3.50087370e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   1.08814463e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,  -3.55244097e-01,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   1.39213280e-01,\n         -3.16176217e-02,   0.00000000e+00,   4.97486291e-01,   0.00000000e+00,\n          0.00000000e+00,   3.66286698e-01,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,  -9.23342670e-03,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          2.56573662e-01,   0.00000000e+00,   9.17364426e-01,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,  -1.75999326e-01,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          6.09102864e-01,   0.00000000e+00,   1.88522504e-01,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   8.54188557e-02,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   1.92068968e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,  -2.83428968e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,  -6.36408811e-02,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   6.21579105e-01,   0.00000000e+00,   0.00000000e+00,\n         -5.21047880e-01,   0.00000000e+00,  -2.00208886e-01,   3.13749660e-01,\n         -4.86758370e-01,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   2.33055931e-01,   0.00000000e+00,   0.00000000e+00,\n         -1.69179139e-01,   0.00000000e+00,  -1.70646121e-01,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   8.10833194e-02,   0.00000000e+00,   3.24678122e-01,\n          0.00000000e+00,   6.38835585e-01,   5.05723589e-01,   0.00000000e+00,\n          1.29359004e-02,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   3.93769822e-01,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   2.43522621e-01,   5.32250551e-02,\n          2.37836239e-01,  -3.04109945e-01,   0.00000000e+00,   5.22113854e-01,\n         -2.74839550e-01,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          2.70077870e-01,   0.00000000e+00]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kvec(score_train,k_ind,900)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,  -5.21994186e-01,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   4.39041917e-01,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   6.33030662e-01,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   4.60918618e-01,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   7.19998188e-01,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          4.71213177e-02,   7.77948461e-01,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,  -6.72711875e-01,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   8.69244692e-01,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,  -5.72418114e-01,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   1.09499829e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   1.06429716e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         -2.07063250e-01,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,  -3.05133963e-01,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   2.15392614e-01,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          6.82716864e-01,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,  -6.61378442e-01,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   1.43051885e-01,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   5.03089569e-01,   0.00000000e+00,\n          0.00000000e+00,  -1.21428346e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,  -8.04987004e-02,   0.00000000e+00,\n         -4.27068986e-01,   0.00000000e+00,   0.00000000e+00,  -2.33292577e-01,\n          2.04175684e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   1.16138511e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   3.28592634e+01,\n         -1.26742048e+00,   7.70482299e-01,   0.00000000e+00,   0.00000000e+00,\n          2.27432251e-01,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   2.14604779e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          1.01346385e+00,   2.11287102e-01,   0.00000000e+00,   2.80507410e-01,\n          7.19823347e-03,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   1.29646197e-01,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   9.22099436e-02,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          7.63877731e-02,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          1.02668752e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,  -1.17007912e-01,   0.00000000e+00,\n          5.47476878e-01,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   6.31049457e-01,   1.39787474e-01,   0.00000000e+00,\n          4.33021859e-01,   7.00717362e-01,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   3.57900198e-01,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   3.32795222e-02,   7.61107569e-01,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   5.46909443e-01,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,  -8.64011099e-01,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,  -5.39936616e-01,   0.00000000e+00,\n          0.00000000e+00,  -1.99855142e-01,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   1.21655283e-01,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   1.26667764e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,  -2.62916196e-01,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   5.70108326e-01,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,  -6.93505856e-01,   0.00000000e+00,\n          0.00000000e+00,  -1.82416247e-01,   7.21614957e-01,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n          1.08659347e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-24.81124227 -12.20737558  -6.01236285  -4.22961506   8.19159786]\n",
      " [  2.84880633  -8.97626212  -3.77494725 -20.55481635  -0.65267763]\n",
      " [ 18.49725774  -0.49910491  12.10706609  13.59896053  19.69734593]\n",
      " [ -5.16672454  -0.62825222   0.28918555  -7.49676265  -7.0854568 ]\n",
      " [-19.10398172  -4.82197861   0.76085909   0.81799849   7.71298224]]\n",
      "[[-5.71060411 -2.48102002 -0.15599939 -0.75286063 -0.04790094]\n",
      " [ 1.08208226 -2.21546958 -0.53473732 -3.72010125 -1.94760456]\n",
      " [ 4.92739943 -0.90350248  1.56479807  2.69635511  2.31691077]\n",
      " [-2.82628778 -0.70687679  0.63864529 -1.17124864 -3.93932912]\n",
      " [-5.26869138 -0.73611451  1.20592093  0.28120331  0.56647573]]\n"
     ]
    }
   ],
   "source": [
    "print(kmat[0:5])\n",
    "print(score_pca[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbb9edc208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(k_ind)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2473, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  39.35988911,    2.29470957,  -28.28188908, ...,   21.58662739,\n",
       "          72.54089086,    4.09421793],\n",
       "       [   2.29470957,   23.99746831,   -8.0463622 , ...,    8.82009926,\n",
       "          -1.69167873,    5.00156789],\n",
       "       [ -28.28188908,   -8.0463622 ,   40.18258126, ...,  -20.71462527,\n",
       "         -55.98606086,    4.13452836],\n",
       "       ..., \n",
       "       [  21.58662739,    8.82009926,  -20.71462527, ...,   38.87677693,\n",
       "          10.01308011,   29.60866848],\n",
       "       [  72.54089086,   -1.69167873,  -55.98606086, ...,   10.01308011,\n",
       "         171.67284391,  -27.68291711],\n",
       "       [   4.09421793,    5.00156789,    4.13452836, ...,   29.60866848,\n",
       "         -27.68291711,   40.57915161]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#So let's just try the sklearn version (which has probably been run through Cython)\n",
    "from sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RadiusNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n             metric_params=None, p=2, radius=1.0, weights='uniform')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Kneighbors does not remove duplicates.\n",
    "Kneigh=KNeighborsRegressor(n_neighbors=10,n_jobs=3)\n",
    "#Radneigh\n",
    "Radneigh=RadiusNeighborsRegressor(n_neighbors=10,n_jobs=3)\n",
    "Kneigh.fit(score_train,score_train)\n",
    "Radneigh.fit(score_train,score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbb9dc0cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb off\n",
    "[neigh2_dist,neigh2_ind]=Radneigh.radius_neighbors(score_train[50000])\n",
    "plt.hist(neigh2_dist[0],bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "[neigh1_dist,neigh1_ind]=Kneigh.kneighbors(score_train[100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#so prediction does not really work out of the box.\n",
    "#Attempts to reshape are not working, since indexing is returning (1,N) array, not (N,) array.\n",
    "#Might be issue for sparse?\n",
    "%pdb off \n",
    "[neigh1_pred]=Kneigh.predict(score_train[100,:])\n",
    "[neigh2_pred]=Radneigh.predict(score_train[100,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9872.,   626.,  2398.,  2127.,   216.,  6436.,  5515.,  4530.,  7824.,\n        6017.,  9914.,   209.,   208.,  1101.,  8995.,  2825.,  8544.,  8339.,\n        7823.,  8036.,  5471.,  2144.,  5475.,  9223.,  5481.,  9427.,  1630.,\n        6981.,  9673.,   809.,  8539.,  8541.,  5050.,  9903.,  3712.,  4333.,\n        7983.,   608.,   447.,  7207.,  7556.,  4961.,  1790.,  4438.,  2838.,\n        1640.,  5625.,  9781.,  7867.,  6583.,  7152.,  9779.,  6512.,  4344.,\n         174.,  2098.,  1761.,  2580.,  1335.,   843.,  5556.,  8175.,  9069.,\n         866.,  9070.,  6430.,  4618.,  9924.,  9926.,  6356.,  9072.,  6355.,\n        1868.,   671.,  5573.,  9500.,  7323.,  8524.,  7996.,  1912.,  6084.,\n        2778.,  5187.,  9847.,  9165.,   620.,  5303.,  5305.,  5183.,  4712.,\n         621.,  9884.,  6086.,  3609.,  9699.,  5202.,  3603.,  3529.,  9463.,\n        4276.])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_ind[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Look into joblib.parallel for some embarassingly parallel computations\n",
    "#(i.e. takes care of threading for me)\n",
    "#Particularly useful for data parallel operations.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Latent Factor Analysis\n",
    "\n",
    "Here we will try to factor the scoring matrix $S=WH$, subject to the cost function:\n",
    "\\begin{align}\n",
    "J &= \\sum_{i,j \\in S_{ij}\\ne 0} \\left(S_{ij} - \\sum_{k=1}^{N_f}W_{ik}H_{kj}\\right)^2\n",
    "&+\\lambda^{(1)} \\sum_{i,j} |W_{ij}|+\\lambda^{(1)} \\sum_{i,j} |H_{ij}|\\nonumber\\\\\n",
    "  &+\\lambda^{(2)} \\sum_{i,j} |W_{ij}|^2+\\lambda^{(2)} \\sum_{i,j} |H_{ij}|^2\\nonumber.\n",
    "  \\end{align}\n",
    "Similar to other problems, the $\\lambda^{(1)}$ regularization enforces sparsity on the problem.  Could we also put in some sort of AIC correction based on the number of degrees of freedom? Something like $N_f^2$?\n",
    "  \n",
    "We can then test the method on how well it recreates existing scores,\n",
    "and make predictions based on the scores for games the user has not rated.  \n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial_{J}}{\\partial W_{nm}} & = \\sum_{ij\\ne 0} \\delta_{in}H_{mj}(S_{ij} -\\sum_{k=1}^{N_f}W_{im}H_{mj})\n",
    "    +\\lambda^{(1)}+\\lambda^{(2)}W_{nm}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Do Stochastic Gradient Descent on\n",
    "W = np.zeros((Nusers,Nf))\n",
    "H = np.zeros((Nf,Ngames))\n",
    "\n",
    "def cost(R,W,H,l1,l2,lN):\n",
    "    \"\"\"cost(R,W,H)\n",
    "    Return cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    J = 0\n",
    "    nrows,ncols=R.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            J+= (R[i,j]- np.dot(W[i,:],H[:,j]))**2 \n",
    "            \n",
    "    J+= l1*np.sum(W) +l1*np.sum(H) + l2*np.sum(W*W)+l2*np.sum(H*H) + lN*Nf*Nf\n",
    "    return J \n",
    "\n",
    "def gradW(R,W,H,l1,l2,lN):\n",
    "    \"\"\"gradW\n",
    "    Calculate derivatives of cost w.r.t. W \n",
    "    \"\"\"\n",
    "    gW = \n",
    "    gW = l1*(W>0) + l2*W\n",
    "    return gW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.740640165451727"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.random.random((3,3))\n",
    "np.sum(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "recommend_sparse.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
