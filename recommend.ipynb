{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Recommendation Engines\n",
    "\n",
    "(This is for the Applied Data Science Group November/December 2017 session.)\n",
    "\n",
    "This notebook tries to build a recommendation engine, which an e-commerce sites would use to recommend other items to you.  Matt Borthwick scraped the data from user reviews at boardgamegeek.com.  This is an initial runthrough to check the quality of the data, and try to play with the distributions.  I'll try to check that the dataset seems sane, check the shape of the distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Possible questions:\n",
    "\n",
    "I did a similar brainstorming exercise (without looking at the data) to what we did in the first week:\n",
    "\n",
    "### Exploratory questions\n",
    "\n",
    "- What is the most popular game?\n",
    "  - Which has the highest average rating?\n",
    "  -  Which has the most reviews?\n",
    "  -  Same for lowest, least reviews\n",
    "\n",
    " - What is the most divisive game?\n",
    "  (Greatest spread in review scores)\n",
    "\n",
    "- Data quality: NA, None, NAN\n",
    "   Number of reviews per user?\n",
    "   Number of reviews per game?\n",
    "   Check scale of review scores\n",
    " - Check distributions of scores\n",
    "\n",
    "## Analysis/Modelling questions\n",
    "\n",
    "- Recommend new games based on similarities with others interests.\n",
    "\n",
    "   Build clustering algorithm based on scores in games.\n",
    "   - Assign each user a vector in Ngame-dim space.\n",
    "   - Find users with similar vectors, based on dot-product.  (K-means or some other clustering)?\n",
    "   - Remove games that are already reviewed, or with negative scores.\n",
    "   - Recommend remaining game with highest score.\n",
    "\n",
    "- User analysis:\n",
    "   Are there multiple audiences here? \"Hardcore\" vs \"casual\" to use the gamer terms.\n",
    "   - How many 1-review users are there? What games do they try out?\n",
    "   - What games do users with multiple reviews enjoy? \n",
    "\n",
    "- Scoring: How will we score/test our recommendations?\n",
    "    - Some sort of cross-validation where we keep a game's scores back, \n",
    "    and try to predict how reviewers will score it, based on their other reviews?\n",
    "\n",
    "    - Is a naive test/train split worthwhile/valid?\n",
    "\n",
    "Handling sparsity:\n",
    "         - Use global function to estimate missing values.  Treat them as the average user.\n",
    "         - use TF-IDF?  Not just most frequent, but ratio of frequency to number of users\n",
    "\n",
    "Latent factor analysis:\n",
    "       - Collaborative filter\n",
    "       -decompose matrix into 2 matrices.  user features vs game features.\n",
    "       - if S_{i,j} is matrix element for user i's score of game j, then\n",
    "       decompose S=UW, where U is N_{user} x N_{hidden}, and W = N_{hidden} x N_{game}.  (This is similar to training word-vectors in natural language processing)\n",
    "       - Train on data.\n",
    "\n",
    "Content-based filtering  (genre tases)\n",
    "user-based filter  (users similar)\n",
    "item-item collaborative filtering.  (game similarities)\n",
    "\n",
    "Try: training on \"elite\" users to define the clusters?\n",
    "\n",
    "Do text analysis on game titles for similarities? (Useful for marketing a game)\n",
    "\n",
    "Associative rule mining\n",
    "\n",
    "Pattern exploration: Which games did people rate at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Simple measures?\n",
    "\n",
    "How to impute missing data?  Average score? (Laplace smoothing from spam?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "I aim to try k-means clustering, and the latent factor analysis approach.\n",
    "I will also try some straightforward collaborative filtering with similarities based on user/game vectors.  I also thought about trying to analyze what games are preferred by reviewers with only a few reviews (n<5) vs many reviews (n>50).\n",
    "\n",
    "Need to think though the optimization criteria, and include appropriate regularization to avoid overfitting.  Maybe use mean-square-error on game scores for games the user has actually reviewed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#standard library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#makes larger plots\n",
    "\n",
    "#save graphics as pdf too (for less revolting exported plots)\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png', 'pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#read in the data.  (13MB or so)\n",
    "#(N.B. I put Matt's header on it's own line, which is skipped, and added the UserID)\n",
    "#initial playing data\n",
    "#df=pd.read_csv('data/boardgame-ratings.csv',skiprows=1)\n",
    "#frequent users\n",
    "\n",
    "df=pd.read_csv('data/boardgame-frequent-users.csv',skiprows=1)\n",
    "\n",
    "#full matrix (2E5 users, 400 games)\n",
    "#df=pd.read_csv('data/boardgame-users.csv',skiprows=1)\n",
    "df.columns=('userID','gameID','rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Matt made a csv file of ids and names  Load into dataframe, put into dict.\n",
    "name_df=pd.read_csv('data/boardgame-titles.csv',index_col=0)\n",
    "name_dict=name_df.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exploratory Analysis\n",
    "\n",
    "I'm going to do a few things:\n",
    "- check for NaN/missing values.\n",
    "- check the scores look right\n",
    "- check the numbers of reviews, and games.\n",
    "- match up the names with the unique gameIDs (I'll find some missing entries here)\n",
    "- plot the number of reviews/user and reviews/game.\n",
    "- check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN 0\n",
      "Max/min scores 1.0 10.0\n"
     ]
    }
   ],
   "source": [
    "#test for NaN\n",
    "nan_array=np.isnan(df.values)\n",
    "print('Number of NaN',np.sum(nan_array))\n",
    "#check scale of review scores.\n",
    "print('Max/min scores',df['rating'].min(),df['rating'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#How many users, how many games?\n",
    "#Find the unique entries in each list\n",
    "users=df['userID'].unique()\n",
    "games=df['gameID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users is: 2473\n",
      "Number of unique games is: 402\n",
      "Total number of reviews is: 528871\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique users is:',len(users))\n",
    "print('Number of unique games is:',len(games))\n",
    "print('Total number of reviews is:',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5319852416043519\n"
     ]
    }
   ],
   "source": [
    "degree_of_sparsity = len(df)/(len(users)*len(games))\n",
    "print(degree_of_sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates:  0\n"
     ]
    }
   ],
   "source": [
    "#check for duplicates\n",
    "dup=df.duplicated()\n",
    "df_dup=df[dup]\n",
    "print('Number of duplicates: ',np.sum(dup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Number of reviews/user and reviews/game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213.85806712494946\n"
     ]
    }
   ],
   "source": [
    "avg_num_reviews=len(df)/len(users)\n",
    "print(avg_num_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So on average, each user reviews 26 games.  Let's try to build a histogram of users with a given number of reviews.  (and then the same with games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#However, this version took a few seconds.\n",
    "user_review_counts=df.groupby(['userID']).count()\n",
    "#note that there really are users with ids going from 1 to 1000, its not a screwup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#find the counts of reviews for each game.\n",
    "game_review_counts=df.groupby(['gameID']).count()\n",
    "#make a list matching up gameIDs and names.  Use that list as a new index\n",
    "new_index=[]\n",
    "i=0\n",
    "for ind in game_review_counts.index:\n",
    "    i+=1\n",
    "    new_index.append(name_dict['title'][ind])\n",
    "\n",
    "game_review_counts.index=new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbc42298d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.figure(figsize=(12,9))\n",
    "plt.figure()\n",
    "plt.hist(user_review_counts.iloc[:,0].values,log=True)\n",
    "plt.xlabel('Number of Reviews')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.title('Reviewer distribution: Number of reviews per user')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So this is a really long-tailed distribution.  It might be nice to look at this histogram on a log-x scale.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbc4d7cef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,9))\n",
    "game_review_counts.iloc[:,1].plot('bar')\n",
    "plt.ylabel('Number of reviews')\n",
    "plt.title('Number of reviews per game')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#make a histogram of number of games with a given number of reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Lets also try to look at the distributions of scores.  I'll try to make a box-plot.\n",
    "That will let me check the distributions in an easy manner.\n",
    "I'll pivot the data frame to make rows users, columns be games, with the entries given by the score. \n",
    "\n",
    "## Boxplots and Transforming the data\n",
    "\n",
    "Rearranging the data to use the gameIDs as columns would make sense for recommendation.\n",
    "For this data set, with 27 dim that's should be no problem. (Another question on what is best to do with thousands of entries).\n",
    "This would also make it easier to look at histograms on a per-game basis.\n",
    "I'm nigh certain pandas has a reshape function to do exactly this.  Pivot maybe?\n",
    "(http://pandas.pydata.org/pandas-docs/stable/reshaping.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gameID  Samurai  Acquire  Elfenland  Bohnanza   Ra  Catan  RoboRally  \\\nuserID                                                                 \n83          NaN      7.0        NaN       NaN  8.0    8.0        8.0   \n119         7.0      7.0        NaN       8.0  8.0    7.0        NaN   \n144         NaN      NaN        NaN       7.0  NaN    6.0        6.0   \n156         7.5      6.5        NaN       7.0  8.0    4.0        7.0   \n186         7.0      NaN        NaN       6.0  8.0    7.0        NaN   \n\ngameID  Can't Stop  Tigris & Euphrates  Liar's Dice        ...          \\\nuserID                                                     ...           \n83             NaN                 8.0          7.0        ...           \n119            6.0                 7.4          7.0        ...           \n144            7.0                 NaN          NaN        ...           \n156            NaN                 8.0          NaN        ...           \n186            8.0                 8.0          NaN        ...           \n\ngameID  Star Wars: Rebellion  Sushi Go Party!  Great Western Trail  Santorini  \\\nuserID                                                                          \n83                       NaN              NaN                  NaN        NaN   \n119                      8.0              NaN                  NaN        NaN   \n144                      9.0              NaN                  8.5        6.0   \n156                      7.5              8.0                  9.5        8.0   \n186                      NaN              8.0                  NaN        8.0   \n\ngameID  Codenames: Pictures  Clank!: A Deck-Building Adventure  Kingdomino  \\\nuserID                                                                       \n83                      NaN                                NaN         NaN   \n119                     NaN                                NaN         NaN   \n144                     NaN                                8.5         7.0   \n156                     NaN                                7.0         NaN   \n186                     NaN                                8.0         NaN   \n\ngameID  Mansions of Madness: Second Edition  Arkham Horror: The Card Game  \\\nuserID                                                                      \n83                                      NaN                           NaN   \n119                                     NaN                           NaN   \n144                                     7.5                           NaN   \n156                                     NaN                           NaN   \n186                                     NaN                           NaN   \n\ngameID  Mechs vs. Minions  \nuserID                     \n83                    NaN  \n119                   NaN  \n144                   8.0  \n156                   7.0  \n186                   8.0  \n\n[5 rows x 402 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a small dataframe for debugging purposes\n",
    "#df_small=df.iloc[0:1000]\n",
    "#make a dense dataframe\n",
    "df_pivot=df.pivot(index='userID',columns='gameID',values='rating')\n",
    "df_pivot=df_pivot.rename(columns=name_dict['title'])\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#df_pivot.to_csv('data/boardgame-ratings-pivot.gz',compression='gzip')\n",
    "#?df.boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f795d609b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "game_review_counts=df_pivot.boxplot(rot=90,grid=False)\n",
    "plt.title('Score distributions by title')\n",
    "plt.ylabel('Rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "These mostly look positive.  Not any radically skewed distributions, like all 1 or all 10.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "(I'll imitate a plot I saw the more experienced folk do at the first finance-data meetup)\n",
    "Try a correlation map based on columns to see how close the score distributions are.\n",
    "I think this intuitively corresponds to: How much are the score distributions in one game similar to another?\n",
    "Running across the rows would yield something analogous for users (but would take an age, since that is a 1E5 x 1E5 matrix).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "corr_mat=df_pivot.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbc42e5f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(corr_mat)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As for building a dataset for recommendations engines, the low correlation is worrisome?  A high correlation implies that everyone likes the same games, in which case there is no space for a skillful recommendation.  Thse are average correlations, rather than user-wise correlations.\n",
    "\n",
    "The low correlation might also be an artifact of lots of reviewers with only a single review. Those entries will have little correlation with anyone else, and may artificially lower the scores?  I also tried keeping only reviews with more than a few scores - it did nothing to change the overall picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py:3033: RuntimeWarning: '<' not supported between instances of 'str' and 'int', sort order is undefined for incomparable objects\n",
      "  return this.join(other, how=how, return_indexers=return_indexers)\n"
     ]
    }
   ],
   "source": [
    "#def reduced_corr(df)\n",
    "Nrow,Ncol=df_pivot.shape\n",
    "\n",
    "rcorr = np.zeros((Ncol,Ncol))\n",
    "Ncorr = np.zeros((Ncol,Ncol))\n",
    "Nreviews = np.zeros(Ncol)\n",
    "mu  = df_pivot.mean(axis=1)\n",
    "std = df_pivot.std(axis=1)\n",
    "\n",
    "#compute scaled dataframe\n",
    "scaled = (df_pivot.subtract(mu,axis='index')/std).values\n",
    "#scaled = ((df_pivot-5.5)/10).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n       ..., \n       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n       [ nan,  nan,  nan, ...,  nan,  nan,  nan]])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Nmax=Ncol\n",
    "#compute correlations between entries, only where both games have been rated.\n",
    "for i in range(Nmax):\n",
    "    mski = ~np.isnan(scaled[:,i])\n",
    "    for j in range(i,Nmax):\n",
    "        mskj = ~np.isnan(scaled[:,j])\n",
    "        msk_tot = mski & mskj\n",
    "        x = scaled[msk_tot,i]\n",
    "        y = scaled[msk_tot,j]\n",
    "        Ncommon=np.sum(msk_tot)\n",
    "        c= np.dot(x,y)/(Ncommon-1)\n",
    "        rcorr[i,j]=c\n",
    "        rcorr[j,i]=c\n",
    "        Ncorr[i,j]=Ncommon\n",
    "        Ncorr[j,i]=Ncommon\n",
    "\n",
    "# #check that the correlation is measuring something like the dot-product between the distributions.\n",
    "# x0=df_pivot.iloc[:,0].values\n",
    "# x1=df_pivot.iloc[:,1].values\n",
    "\n",
    "# x0_mu=np.nanmean(x0)\n",
    "# x1_mu=np.nanmean(x1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#find number of reviews within each integer bin size\n",
    "def game_hist(df):\n",
    "    df_counts=pd.DataFrame()\n",
    "    for i in range(1,11):\n",
    "        Ntot=(df.round()==i).sum(axis=0).astype(int)\n",
    "        df_counts=df_counts.append(Ntot,ignore_index=True)\n",
    "    df_counts.index=np.arange(1,11)\n",
    "    df_counts=df_counts/np.sum(df_counts)\n",
    "    return df_counts\n",
    "\n",
    "#compute histograms for dot products. \n",
    "def raw_hist(df,Nbins=10):\n",
    "    df_counts=pd.DataFrame()\n",
    "    imin=-1\n",
    "    imax=1\n",
    "    dx = (imax-imin)/Nbins\n",
    "    for i in range(Nbins):\n",
    "        i0 = i*dx\n",
    "        i1 = (i+1)*dx\n",
    "        Ntot=(df>i0 & df<i1).sum(axis=0).astype(int)\n",
    "        df_counts=df_counts.append(Ntot,ignore_index=True)\n",
    "    df_counts.index=np.linspace(imin,imax,Nbins)\n",
    "    df_counts=df_counts/np.sum(df_counts)\n",
    "    return df_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_counts=game_hist(df_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Is there much similarity in the score distributions?  Not really useful question to ask.\n",
    "But the histogram plot is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "?plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbc433b550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(np.log(df_counts),aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Games')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The above image is a plot of the score densities for all of the games.  I'm just trying to get a sense of what the score distributions look like.\n",
    "Most of the games are scored within 6-8.\n",
    "\n",
    "It might be interesting to identify games by their variance?  Which games is there a consensus on, and which games are divisive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbbd902c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(corr_mat)\n",
    "plt.colorbar()\n",
    "plt.title('Pandas Correlation')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(rcorr)\n",
    "plt.colorbar()\n",
    "plt.title('\"Corrected\" Correlation')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(Ncorr)\n",
    "plt.colorbar()\n",
    "plt.title('Fraction of Common Reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7efbbdab7a20>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbc5ff8b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=5\n",
    "j=90\n",
    "plt.scatter(df_pivot.iloc[:,i],df_pivot.iloc[:,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The average pair-wise correlations between user's opinions of a game are really low.\n",
    "Maybe a cluster analysis might yield something non-trivial?\n",
    "This says on average, it's hard to guess what a person will think (in terms of variations from the mean).\n",
    "\n",
    "Looking over the numbers, the correlations seem to be based on 1000 ratings in common for any pair of games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##make logical array for actual reviews.\n",
    "#ntot=np.sum(df_pivot>0,axis=1)\n",
    "##only keep those with more than 6 review.\n",
    "#keep_msk=ntot>20\n",
    "#df_pivot2=df_pivot[keep_msk]\n",
    "#len(df_pivot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Conclusions regarding state of data\n",
    "\n",
    "- The number of reviews per user is skewed towards new folks (not unreasonable, given how few people can stick at something as time intensive as playing and reviewing board games).\n",
    "\n",
    "- Looking at the box-plots, the scores seem fairly high, which tallies with what Matt said about picking popular games.  There doesn't seem anything obviously wrong with the distributions (all zero, or all 10s).\n",
    "\n",
    "- I think for analysis, it would be beneficial to reshape the dataframe/array, but that is probably best left to the participants, as is removing any data with few reviews.  I used \"pivot\" to transform the gameID column, into a new set of columns, while keeping the reviewers as rows.  This will make building feature vectors straightforward.\n",
    "\n",
    "- I tried building up some histograms via looping, and it was indeed quite slow.  In contrast, the arcane, built-in functions (groupby) are super fast.  The smaller dataset should allow accessibility to new people, while the full dataset is quite manageable if you find the right set of functions.  I haven't done any actual machine-learning with this yet, so maybe I'll eat my words about \"manageable\".\n",
    "\n",
    "- The correlation plots seem to show a small, positive correlation.  Is this even a sensible measure?  Its something like the overlap between the shapes of the ratings distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Splitting into training/test\n",
    "\n",
    "I'm going to manually force a training/test split.  I'm going to randomly select 10% of users, and 10% of games.  To my mind any measure of similarity shuld be able to detect generic tastes, and be able to predict how well a  \n",
    "Our goal is to recommend games people will like.  We can do this by holding back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Ngames=len(games)\n",
    "Nusers=len(users)\n",
    "\n",
    "#make a list of uniform random numbers (times appropriate lengths)\n",
    "game_fixed=[12,14]  #I've at least heard of Agricola, (rules heavy \"Eurogame\")\n",
    "game_ix=np.random.random(size=Ngames)<0.1\n",
    "user_ix=np.random.random(size=Nusers)<0.1\n",
    "\n",
    "#keep testing examples from to test new users on old games, and new games on old users.\n",
    "df_game_test=df_pivot.iloc[~user_ix,game_ix].copy()\n",
    "df_user_test=df_pivot.iloc[user_ix,game_ix].copy()\n",
    "\n",
    "#keep only the non-testing examples\n",
    "df_train = df_pivot.iloc[~user_ix,~game_ix]\n",
    "#We are then free to try predicting \"new users\" scores (given a way of decomposing new users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The \"game\" test is for feature vectors trained on a set of users, can we get the correct rating on games they rated.\n",
    "Second, given some \"new\" user, can we get the correct scores.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2235, 363)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_counts=game_hist(df_train)\n",
    "plt.imshow(df_counts/np.sum(df_counts))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Similarity vectors.\n",
    "\n",
    "Let's now try to make vectors for each person and game, and measure the distance between people?\n",
    "(I think this realy needs to be centered.  Otherwise, there seems to be a really high left over correlation between users opinions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df0=df_train.values\n",
    "mu = df_train.mean(axis=1)\n",
    "sd = df_train.std(axis=1)\n",
    "#because broadcasting only goes so far.\n",
    "#need to use direct ops to scale\n",
    "scaled = df_train.sub(mu,axis=0).div(sd,axis=0)\n",
    "\n",
    "#scale features to be on -1,1\n",
    "scaled = (df_train-5.5)/10\n",
    "\n",
    "#df2=(df2-mu)#/df_train.std(axis=1)\n",
    "#df2 = df2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "nan_msk=np.isnan(scaled)\n",
    "scaled[nan_msk]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Samurai', 'Acquire', 'Elfenland', 'Bohnanza', 'Ra', 'Catan',\n       'RoboRally', 'Can't Stop', 'Tigris & Euphrates', 'Liar's Dice',\n       ...\n       'Viticulture Essential Edition', 'Star Wars: Rebellion',\n       'Sushi Go Party!', 'Great Western Trail', 'Santorini',\n       'Clank!: A Deck-Building Adventure', 'Kingdomino',\n       'Mansions of Madness: Second Edition', 'Arkham Horror: The Card Game',\n       'Mechs vs. Minions'],\n      dtype='object', name='gameID', length=361)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df3=np.matmul(scaled,scaled.T)\n",
    "df3_d=np.diag(df3)\n",
    "df3=df3/df3_d\n",
    "#try to see small changes\n",
    "one_msk=df3==1\n",
    "df3[one_msk]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbbdd1df98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df3\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(df3)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "##Try some clustering to identify populations.\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# #Initial attempt at clustering with raw review scores\n",
    "# df_new=df_train.copy()\n",
    "# #convert dataframe into \n",
    "# df_mat=df_new.values\n",
    "# nan_msk=np.isnan(df_mat)\n",
    "# df_mat[nan_msk]=0\n",
    "\n",
    "#New attempt at clustering with raw review scores\n",
    "#convert dataframe into \n",
    "df_mat=scaled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Try minibatch Kmeans (as recommended)\n",
    "Nclasses=20\n",
    "km=KMeans(n_clusters=Nclasses,n_jobs=3)\n",
    "km.fit(df_mat.values)\n",
    "ypred=km.predict(df_mat.values)\n",
    "df_mat['Class']=ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbbc052b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#now to try visualizing\n",
    "plt.figure(figsize=(15,10))\n",
    "Ncol=4\n",
    "Nrows=Nclasses//Ncol+1\n",
    "for i in range(Nclasses):\n",
    "    plt.subplot(Nrows,Ncol,i+1)\n",
    "    #determine which rows are in a given class.\n",
    "    msk=df_mat['Class']==i\n",
    "    #then plot the scores for each class (from unscaled data)\n",
    "    d0=game_hist(df_train[msk])\n",
    "    plt.imshow(d0,aspect='auto')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "?KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So this is once again complete trash.\n",
    "\n",
    "How else to visualize these?  Try plotting the means within a given cluster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363146.62347590784"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cent=km.cluster_centers_\n",
    "inert=km.inertia_\n",
    "\n",
    "inert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Principle Components Analysis\n",
    "\n",
    "Let's try a PCA on this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=10, random_state=None,\n  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df_new=scaled.copy()\n",
    "#convert dataframe into \n",
    "df_mat=df_new.values\n",
    "nan_msk=np.isnan(df_mat)\n",
    "df_mat[nan_msk]=0\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(df_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbc4e1ca90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's look at the amount of explained variance.\n",
    "plt.figure()\n",
    "plt.plot(pca.explained_variance_,'-x')\n",
    "plt.axis([0,20,0,.6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "?pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbc4e5bef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#try visualizing the components\n",
    "pca_comp = pca.components_\n",
    "\n",
    "plt.figure()\n",
    "for i in range(5):\n",
    "    plt.plot(pca_comp[i,:])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbc4e549b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the minimum rating given out by each user.  \n",
    "plt.hist(np.std(df_mat,axis=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As a first pass, let's try 4 principle components.  I'd like to visualize these components.\n",
    "This has selected out 361 features.  These features could be said to correspond to taste profiles? - how much did each class like a given game.\n",
    "Decompose each user into a superposition of each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_decomp=np.dot(df_mat,pca_comp.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.18892319, -0.02312199,  0.14230487, ..., -0.21633113,  0.06926489,\n        -0.33280267],\n       [-1.33620348, -0.23943944,  0.74373358, ...,  0.07377366,  0.10761448,\n         0.00955454],\n       [-1.89127898, -0.09527716,  0.22349309, ...,  0.0526667 ,  0.10148451,\n         0.02895422],\n       ..., \n       [-1.79521595,  0.96701436,  0.33837945, ...,  0.17229478,  0.08216916,\n         0.07340455],\n       [-3.04566306, -0.29459339,  0.75142233, ..., -0.32349976,  0.27358695,\n         0.13498643],\n       [-1.7444841 ,  0.70695794,  0.63127698, ..., -0.18157476, -0.2584679 ,\n        -0.06526969]])"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_decomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb7d1c6d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "# plt.imshow(df_decomp.T[0:10],aspect='auto')\n",
    "# plt.colorbar()\n",
    "for i in range(5):\n",
    "    plt.plot(df_decomp[:,i],label=str(i))\n",
    "plt.legend()            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.49272035,   2.39836865,   2.43987682, ...,  -1.39935203,\n         -1.02021922,  -1.17207476],\n       [  2.39836865,  11.57305121,   1.89213868, ...,  -1.61709379,\n         -1.3275407 ,  -1.0037423 ],\n       [  2.43987682,   1.89213868,  11.36776863, ...,  -0.75502413,\n         -1.2557539 ,  -1.14314742],\n       ..., \n       [ -1.39935203,  -1.61709379,  -0.75502413, ...,  12.31648846,\n          4.88844605,   3.05726093],\n       [ -1.02021922,  -1.3275407 ,  -1.2557539 , ...,   4.88844605,\n         10.99727437,   3.03394189],\n       [ -1.17207476,  -1.0037423 ,  -1.14314742, ...,   3.05726093,\n          3.03394189,  12.08365687]])"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#Vector embeddings\n",
    "\n",
    "I'd like to reprise an approach borrowed from text-mining for training word vectors.  We'd like to extract information both on the similarities of\n",
    "users, and games.  As suggested, a matrix factorization method.  We'd train the embeddings on some subset of the matrix.  New games are treated on the assumption that similar ratings.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "recommend.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
